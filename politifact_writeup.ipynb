{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c44863-2abf-47a2-881f-df6922804afd",
   "metadata": {},
   "source": [
    "### Code Review\n",
    "- Basel\n",
    "    - Tried to help us with our models but also could not figure out what was going on with TensorFlow\n",
    "    - showed me about the Hugging face library and those datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d41f4db-a4dd-4756-8652-042567b99e9c",
   "metadata": {},
   "source": [
    "# Politifact Writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bf4f5e-396a-48ca-a292-96222de50ae8",
   "metadata": {},
   "source": [
    "## Initial Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c0a64c-778f-444e-bbed-45430b50ce9f",
   "metadata": {},
   "source": [
    "    Model: \"sequential\"\n",
    "    _________________________________________________________________\n",
    "    Layer (type)                 Output Shape              Param #   \n",
    "    =================================================================\n",
    "    embedding (Embedding)        (None, None, 300)         5495100   \n",
    "    _________________________________________________________________\n",
    "    lstm (LSTM)                  (None, None, 128)         219648    \n",
    "    _________________________________________________________________\n",
    "    global_max_pooling1d (Global (None, 128)               0         \n",
    "    _________________________________________________________________\n",
    "    dense (Dense)                (None, 64)                8256      \n",
    "    _________________________________________________________________\n",
    "    dropout (Dropout)            (None, 64)                0         \n",
    "    _________________________________________________________________\n",
    "    dense_1 (Dense)              (None, 6)                 390       \n",
    "    =================================================================\n",
    "    Total params: 5,723,394\n",
    "    Trainable params: 5,723,394\n",
    "    Non-trainable params: 0\n",
    "    _________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af314a7-2426-4c51-b3f7-fb1c20b5fbd7",
   "metadata": {},
   "source": [
    "    batch_size=128\n",
    "    epochs=10\n",
    "    \n",
    "    529/529 [==============================] - 5s 9ms/step - loss: 0.7062 - accuracy: 0.9037\n",
    "    Train loss: 0.7062075734138489\n",
    "    Train accuracy: 0.9037290811538696\n",
    "    133/133 [==============================] - 1s 9ms/step - loss: 2.5756 - accuracy: 0.2579\n",
    "    Test loss: 2.5755887031555176\n",
    "    Test accuracy: 0.25785866379737854\n",
    "    \n",
    "    optimizer=Adam\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32a78a7-a55b-4e80-9519-1b32cd74b9a8",
   "metadata": {},
   "source": [
    "Initial setup of the model shows that there is extreme overfitting occurring, so need to figure out how to reduce this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2be1ea-2389-4501-933a-b1f068cce436",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf957e-d863-4fc6-8679-dea90eb86d9a",
   "metadata": {},
   "source": [
    "There were two ways to preprocess the data that we used. We used tokenization from the glove.6B.300 model which already has embeddings for 400,000 words in the English dictionary. In order to use this we map the each word with it's embedding in a dictionary. Then we find the weights of said word and return the weights in matrix form of n x 300 dimensions.\n",
    "\n",
    "The other way to preprocess the data was using an already trained embedder found on TensorFlow Hub. We used https://tfhub.dev/google/universal-sentence-encoder/4 Google's universal-sentence-encoder in order to process the data. It was designed on a DAN enconder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c01c54d-c11a-42f4-b056-5e8e15b4604c",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ec90bb-82bc-4b8e-ba12-a632aa25d4ba",
   "metadata": {},
   "source": [
    "Attempted multiple things when trying to test. Used many different architectures such as adding more LSTM layers. Added more dense layers, added global pooling, dropout. Every type of layer imaginable; however, the model would stop training at one point and begin to increase loss on validation set, as shown below.\n",
    "\n",
    "    Epoch 1/10\n",
    "    15/15 [==============================] - 8s 200ms/step - loss: 1.7598 - accuracy: 0.2612 - val_loss: 1.7364 - val_accuracy: 0.2575\n",
    "    Epoch 2/10\n",
    "    15/15 [==============================] - 2s 160ms/step - loss: 1.6875 - accuracy: 0.2807 - val_loss: 1.6542 - val_accuracy: 0.2983\n",
    "    Epoch 3/10\n",
    "    15/15 [==============================] - 2s 160ms/step - loss: 1.5945 - accuracy: 0.3161 - val_loss: 1.6272 - val_accuracy: 0.3042\n",
    "    Epoch 4/10\n",
    "    15/15 [==============================] - 2s 159ms/step - loss: 1.4922 - accuracy: 0.3501 - val_loss: 1.6435 - val_accuracy: 0.3255\n",
    "    Epoch 5/10\n",
    "    15/15 [==============================] - 2s 158ms/step - loss: 1.3285 - accuracy: 0.4238 - val_loss: 1.7706 - val_accuracy: 0.3119\n",
    "    Epoch 6/10\n",
    "    15/15 [==============================] - 2s 160ms/step - loss: 1.0922 - accuracy: 0.5283 - val_loss: 2.1103 - val_accuracy: 0.3048\n",
    "    Epoch 7/10\n",
    "    15/15 [==============================] - 2s 159ms/step - loss: 0.8241 - accuracy: 0.6491 - val_loss: 2.7432 - val_accuracy: 0.2758\n",
    "    Epoch 8/10\n",
    "    15/15 [==============================] - 2s 159ms/step - loss: 0.5962 - accuracy: 0.7398 - val_loss: 3.3854 - val_accuracy: 0.2989\n",
    "    Epoch 9/10\n",
    "    15/15 [==============================] - 2s 159ms/step - loss: 0.4022 - accuracy: 0.8300 - val_loss: 4.3971 - val_accuracy: 0.3077\n",
    "    Epoch 10/10\n",
    "    15/15 [==============================] - 2s 158ms/step - loss: 0.2413 - accuracy: 0.9073 - val_loss: 5.4773 - val_accuracy: 0.2995\n",
    "    [22]:\n",
    "    <tensorflow.python.keras.callbacks.History at 0x1551db144ca0>\n",
    "    \n",
    "We noticed that if we do not use \"from_logits=True\" in the loss, then the results would be even worse as the accuracy would decrease.\n",
    "\n",
    "However, with the two types of encoding and embedding the words. We still did not seem to be able to make much progress. The model was able to memorize the training data well, but did not seem to learn as well. There were points where the validation loss would decrease, but not as quickly as the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51c2513-1b08-4e5a-a854-8a1508e26955",
   "metadata": {
    "tags": []
   },
   "source": [
    "We attempted to use the BERT encoder but that did not work because TensorFlow would not download the TensorFLow library.\n",
    "\n",
    "However, we did use other encoders from TensorFlow's hub library and that did not create any other improvements in the model.\n",
    "\n",
    "## Future Work\n",
    "Possibly the issue arises from not processing the data enough, there are methods available to make the data more learnable for the machine. These include changing words like running to run, or getting rid of words such as \"and\", \"to\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a61bd3-ecb3-4053-be16-96ef5717f3ec",
   "metadata": {},
   "source": [
    "## Shortcomings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a6191f-73fa-4fa8-a6d1-73ed2e7b6f8d",
   "metadata": {},
   "source": [
    "A few issues arised when trying to accomplish this project. When we tried to use TensorFlow's text library for preprocessing or to try to use BERT, it would never load in and broke Kostas's TensorFlow on Jupyter. This prohibited us from being able to make use of TensorFlow's immense preprocessing library for text specific data. We believe this played a role in not building a very usable model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea8207d-a78a-461a-ab4a-77fbeb17d19d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The model that we decided as our best was our intial run because the other attempts to change it prompted us to have decreasing accuracy as epochs continued. We decided that having a working TensorFlow library would help us do more to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013a6fee-51b2-40f6-8ab1-a142374cb8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "python3-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
