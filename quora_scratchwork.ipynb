{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88360b99-a52a-4d6b-95ae-b162dd5da005",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "/home/DAVIDSON/ilnavani/.venvs/TensorFlow-GPU/lib/python3.8/site-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow15OpKernelContext21CtxFailureWithWarningEPKciRKN3tsl6StatusE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m/opt/pub/eb/apps/cascadelake/TensorFlow/2.5.0-fosscuda-2020b/lib/python3.8/site-packages/tensorflow/__init__.py:444\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    442\u001b[0m _main_dir \u001b[38;5;241m=\u001b[39m _os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_s, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow/core/kernels\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(_main_dir):\n\u001b[0;32m--> 444\u001b[0m   \u001b[43m_ll\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_main_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# Load third party dynamic kernels.\u001b[39;00m\n\u001b[1;32m    447\u001b[0m _plugin_dir \u001b[38;5;241m=\u001b[39m _os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_s, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow-plugins\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/pub/eb/apps/cascadelake/TensorFlow/2.5.0-fosscuda-2020b/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py:154\u001b[0m, in \u001b[0;36mload_library\u001b[0;34m(library_location)\u001b[0m\n\u001b[1;32m    151\u001b[0m     kernel_libraries \u001b[38;5;241m=\u001b[39m [library_location]\n\u001b[1;32m    153\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m lib \u001b[38;5;129;01min\u001b[39;00m kernel_libraries:\n\u001b[0;32m--> 154\u001b[0m     \u001b[43mpy_tf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_LoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    158\u001b[0m       errno\u001b[38;5;241m.\u001b[39mENOENT,\n\u001b[1;32m    159\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe file or folder to load kernel libraries from does not exist.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    160\u001b[0m       library_location)\n",
      "\u001b[0;31mNotFoundError\u001b[0m: /home/DAVIDSON/ilnavani/.venvs/TensorFlow-GPU/lib/python3.8/site-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow15OpKernelContext21CtxFailureWithWarningEPKciRKN3tsl6StatusE"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import math\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a2ba910f-17d3-4ba0-96a6-5fbbc6ee6df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/DAVIDSON/brwiedenbeck/public/NLP/quora.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "eaed01c8-3200-4c6e-80d2-182efddf48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, train_size=0.80, stratify = df['target'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "03bf790f-a18a-4782-b9ad-18f10abe5e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(train_data, train_size=0.80, stratify = train_data['target'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24b2ecbe-70ad-4dcd-8359-d6bd28a5744e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(835917, 3)\n",
      "0    784199\n",
      "1     51718\n",
      "Name: target, dtype: int64\n",
      "(208980, 3)\n",
      "0    196050\n",
      "1     12930\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(train_data['target'].value_counts())\n",
    "print(val_data.shape)\n",
    "print(val_data['target'].value_counts())\n",
    "print(test_data.shape)\n",
    "print(test_data['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d7b9c0a-14de-40cd-b68c-3d0174d90728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant text and labels \n",
    "text = df['question_text']\n",
    "labels = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb92772c-88bb-4b04-8108-1dbed5aef899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test data (80-20 split)\n",
    "train_data, test_data = train_test_split(text, train_size=0.80, random_state=42)\n",
    "train_labels, test_labels = train_test_split(labels, train_size=0.80, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ab47edc-2920-457a-bd93-50a6dcd7b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract validation set (20%) from training data\n",
    "train_data, val_data = train_test_split(train_data, train_size=0.80, random_state=42)\n",
    "train_labels, val_labels = train_test_split(train_labels, train_size=0.80, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee7f8ee9-01e2-42fa-9765-05d6566bf69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"validation_labels\", val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e426ea-0385-4508-8b3f-c6e5a7cb56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a debugging set from train data (1% of data)\n",
    "#train_data, debug_data = train_test_split(train_data, train_size=0.99, random_state=42)\n",
    "#train_labels, debug_labels = train_test_split(train_labels, train_size=0.99, random_state=42)\n",
    "#1630 gives batches of 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18861bfa-e1a2-483a-8cf5-d218cd988c83",
   "metadata": {},
   "source": [
    "### Preprocessing Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04cf2d5c-27df-4e96-b45a-6ef91641d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into 10 batches\n",
    "train_data = np.array_split(train_data, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c66cf51-6d6a-4d0c-848f-da7db9dbf1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69660"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2713033d-4d76-460e-85fd-280ba796429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array_split(train_labels, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f6b7d8b-a4de-49a1-8a4a-baa51613f4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69660"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0be1fdd5-12af-4884-b8ff-f4f289e49db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"train_labels_0\", train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "384924af-4cdf-4912-87e7-694566d494cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get glove embeddings\n",
    "# \"embedding\" is a dictionary of raw embeddings of words\n",
    "f = open(\"glove.6B.100d.txt\")\n",
    "lines = f.readlines()\n",
    "embedding = dict()\n",
    "for line in lines:\n",
    "        parts = line.split()\n",
    "        embedding[parts[0]] = np.asarray(parts[1:], dtype='float32')\n",
    "        #, dtype='float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0fc46a49-1145-432b-8254-b893b380dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_embedding(train_data):\n",
    "\n",
    "    tokens = []\n",
    "    for elem in train_data:\n",
    "        token = text_to_word_sequence(elem)\n",
    "        tokens.append(token) \n",
    "\n",
    "    train_embedding = []\n",
    "    for question in tokens:\n",
    "        matrix = np.zeros((len(question), 100))\n",
    "        for index, word in enumerate(question):\n",
    "            vector_rep = embedding.get(word)\n",
    "            if vector_rep is not None:\n",
    "                matrix[index] = vector_rep\n",
    "            train_embedding.append(matrix)\n",
    "    #train_embedding = np.array(train_embedding, dtype=object)\n",
    "    train_embedding = pad_sequences(train_embedding, padding = 'post', dtype='float64')\n",
    "\n",
    "#tokens.append(batch_tokens)\n",
    "#tokens = pd.Series(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "33ecb7b6-a147-45b1-b6a6-e0b31c1718fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_embedding(train_data):\n",
    "    \n",
    "    tokens = text_to_word_sequence(train_data)\n",
    "    matrix = np.zeros((len(tokens), 100))\n",
    "    for index, word in enumerate(tokens):\n",
    "        vector_rep = embedding.get(word)\n",
    "        if vector_rep is not None:\n",
    "            matrix[index] = vector_rep\n",
    "    matrix = np.array(matrix, dtype=object)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "d9bae6dc-362f-4c8f-abf2-ed0753a006bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = glove_embedding(train_data.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b9fb40ec-8ee2-4836-9084-6636288b96cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(embedding1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "185dde8d-9c09-4c8d-9f67-16155c49b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"train_data_0\", train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b562af2-b69f-4809-b926-f20903db24ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels \n",
    "train_embedding = []\n",
    "for question in tokens:\n",
    "    matrix = np.zeros((len(question), 100))\n",
    "    for index, word in enumerate(question):\n",
    "        vector_rep = embedding.get(word)\n",
    "        if vector_rep is not None:\n",
    "            matrix[index] = vector_rep\n",
    "    train_embedding.append(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2329e65a-4987-47f9-b1d9-e3c53f63bfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of embeddings for each question in the training set\n",
    "train_embedding = []\n",
    "for batch in tokens:\n",
    "    batch_embedding = []\n",
    "    for question in batch:\n",
    "        matrix = np.zeros((len(question), 100))\n",
    "        for index, word in enumerate(question):\n",
    "            vector_rep = embedding.get(word)\n",
    "            if vector_rep is not None:\n",
    "                matrix[index] = vector_rep\n",
    "        batch_embedding.append(matrix)\n",
    "    batch_embedding = pad_sequences(batch_embedding)\n",
    "    train_embedding.append(batch_embedding)\n",
    "    \n",
    "# Padding to make all sequences the same length\n",
    "#train_embedding = pad_sequences(train_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee55a0e-165b-400d-a409-a41979b531fa",
   "metadata": {},
   "source": [
    "### Preprocessing Debug set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d48a926-67aa-447f-b7ae-e72688d17a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert questions to word tokens\n",
    "debug_tokens = []\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "for elem in debug_data:\n",
    "    tokens = text_to_word_sequence(elem)\n",
    "    debug_tokens.append(tokens)\n",
    "\n",
    "debug_tokens = pd.Series(debug_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2cf4f8-4d41-4862-b31a-e8c64bf9c3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each question to numeric tokens  -- not using this right now\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(debug_data)\n",
    "debug_sequences = tokenizer.texts_to_sequences(debug_data)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94ca1d27-f9f4-43b6-8920-45abc082ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get glove embeddings\n",
    "# Embedding is a dictionary of raw embeddings of words\n",
    "f = open(\"glove.6B.100d.txt\")\n",
    "lines = f.readlines()\n",
    "embedding = dict()\n",
    "for line in lines:\n",
    "        parts = line.split()\n",
    "        embedding[parts[0]] = np.asarray(parts[1:], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f318c2-608a-4307-9ab1-7821e57b09f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of embeddings for each question in the debug set\n",
    "debug_embedding = []\n",
    "for question in debug_tokens:\n",
    "    matrix = np.zeros((len(question), 100))\n",
    "    for index, word in enumerate(question):\n",
    "        vector_rep = embedding.get(word)\n",
    "        if vector_rep is not None:\n",
    "            matrix[index] = vector_rep\n",
    "    debug_embedding.append(matrix)\n",
    "    \n",
    "# Padding to make all sequences the same length\n",
    "debug_embedding = pad_sequences(debug_embedding, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725bf78a-57ea-423c-ad7b-9bcfc6f2f4fd",
   "metadata": {},
   "source": [
    "### Preprocessing Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ec878ff-5a9c-401e-a937-6bfa5f3df67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each validation question to word tokens\n",
    "val_tokens = []\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "for elem in val_data:\n",
    "    tokens = text_to_word_sequence(elem)\n",
    "    val_tokens.append(tokens)\n",
    "\n",
    "val_tokens = pd.Series(val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8be3bdc5-8fae-4db7-879c-794c5d51eef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208980"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2126c2f9-8958-4e67-afeb-cd51ee9a6011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of embeddings for each question in the validation set\n",
    "val_embedding = []\n",
    "for question in val_tokens:\n",
    "    matrix = np.zeros((len(question), 100))\n",
    "    for index, word in enumerate(question):\n",
    "        vector_rep = embedding.get(word)\n",
    "        if vector_rep is not None:\n",
    "            matrix[index] = vector_rep\n",
    "    val_embedding.append(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7027d442-c71a-4480-9c85-eb12afcb640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embedding = np.array(val_embedding, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4edd4d-49a9-4d7b-ad9d-36d383647006",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embedding = pad_sequences(val_embedding, padding = 'post', dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb6b58-964c-4b27-85d4-57d3eeb813ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give embedded questions as input to model. You don't need the embedding layer anymore.\n",
    "# Embedding layer is: #model.add(tf.keras.layers.Embedding(vocab_size, 300, weights = [debug_embedding], input_length=300, trainable = False))\n",
    "# No need to pad sequences for LSTM bec it takes in one word at a time. You may need to pad for transformers?\n",
    "# Set 'return_sequences = True' for an LSTM that is feeding into another LSTM\n",
    "# Very high accuracies in the 95% range are plausible \n",
    "# For this dataset there are a lot more sincere questions than insincere\n",
    "# So create a confusion matrix to see if we are in fact making good predictions on the insincere questions\n",
    "# Loop over the epochs and loop over the files\n",
    "# regularizers and dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ef294-d550-4d06-821d-2fc74b8496dd",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b96e5db6-b43b-4b17-981c-428cc9b9af84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(file = \"train_data_0.npy\", allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c061d249-9db7-42de-a2a9-9c1abfd16863",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embedding = np.load(file = \"validation_data.npy\", allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f65d664-66cc-45ca-9199-a91de9d30c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208980"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0924f4b7-ef7b-4d49-8ed3-fca9725f5019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_embedding[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6b09ab4-c4bc-44cc-9769-90732d64e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embedding = np.array_split(val_embedding, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb20ada2-5e0a-4bce-b9cd-8bb829d48693",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in range(208980):\n",
    "    val_embedding[elem] = pad_sequences(val_embedding[elem], padding = 'post', dtype='float64', maxlen = 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9431b41-41e7-432e-aefd-e8e9b6f46b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embedding[0] = pad_sequences(val_embedding[0], maxlen = 63, padding = 'post', dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc4cd7a-e710-44e5-93a2-306abde91c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 10000\n",
    "val_embedding = []\n",
    "for question in val_data[\"question_text\"][:val_size]:\n",
    "    val_embedding.append(glove_embedding(question))\n",
    "    \n",
    "val_embedding = np.array(val_embedding, dtype=object)\n",
    "val_embedding = pad_sequences(val_embedding, padding = 'post', dtype = 'float32', maxlen = 100)\n",
    "val_labels = np.array(val_data[\"target\"][:val_size], dtype='int32')\n",
    "\n",
    "validation_data=(val_embedding, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b7181-b612-4704-9996-cf31d4554517",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648a031c-76c3-4b3f-8814-20c21ebd8efe",
   "metadata": {},
   "source": [
    "Outer loop goes through epochs. Inner loop goes through files. \n",
    "Within each inner loop, load each file (generate string name by changing the index) and fit it for 1 epoch (so you only have one file in memory at a time) \n",
    "Afterwards run through validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "0eb5c128-149c-4c44-8aa1-ef18ce7ee644",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.LSTM(units = 128))\n",
    "model.add(tf.keras.layers.Dense(16, activation = 'sigmoid'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "#model.add(tf.keras.layers.Dense(2, activation = 'softmax'))\n",
    "model.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "3051e16c-1d0e-4303-850d-77e4b53226b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add((tf.keras.layers.LSTM(128,return_sequences=True)))\n",
    "model.add(tf.keras.layers.GlobalMaxPool1D())\n",
    "model.add(tf.keras.layers.Dense(64,activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0368ce18-f9b4-4f02-a975-64f29ee0c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "62e60e88-3345-4dfb-9aa1-adf683a8754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(0.001), loss=\"binary_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c48f7e41-d81d-45b7-bf80-32d3ae163f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(), \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=[Accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "8efd890f-e82c-4c28-af2b-ee809d3f8465",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"target\"] = keras.utils.to_categorical(train_data[\"target\"], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a26d89bb-76e2-4714-939d-4753edca562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data[\"target\"] = keras.utils.to_categorical(val_data[\"target\"], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022aad68-2995-4d9f-bb1e-7f68c5697276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#callback = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 3, restore_best_weights = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "09f2d9de-3be2-41f4-97e7-60d4e668ab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator when reading one file at a time \n",
    "batch_size = 256\n",
    "\n",
    "# create an generator for the training set\n",
    "def batch_gen(train_df, train_labels):\n",
    "    # number of batches\n",
    "    n_batches = math.ceil(len(train_df) / batch_size)\n",
    "    while True: \n",
    "        #train_df = train_df.sample(frac=1.)  # Shuffle the data.\n",
    "        for i in range(n_batches):\n",
    "            texts = train_df[i*batch_size:(i+1)*batch_size] \n",
    "            yield texts, train_labels[i*batch_size:(i+1)*batch_size] # generator: embedding and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "f486dbb3-de28-4a09-ad06-4ebe7868cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator for entire dataset \n",
    "batch_size = 256\n",
    "\n",
    "def train_gen(train_data):\n",
    "    num_batches = math.ceil(len(train_data) / batch_size)\n",
    "    while True: \n",
    "        train_data = train_data.sample(frac=1.) #shuffle the data\n",
    "        for i in range(num_batches):\n",
    "            train_questions = []\n",
    "            batch = train_data.iloc[i * batch_size: (i + 1) * batch_size, 1]\n",
    "            for question in batch:\n",
    "                train_questions.append(glove_embedding(question))\n",
    "            train_questions = np.array(train_questions, dtype = object)\n",
    "            #text_arr.append([glove_embedding(text) for text in texts], dtype=object)\n",
    "            #text_arr = np.array([glove_embedding(text) for text in texts], dtype=object)\n",
    "            train_questions = pad_sequences(train_questions, padding = 'post', dtype = 'float32', maxlen = 100)\n",
    "            #text_arr = tf.cast(text_arr, dtype=tf.int32)\n",
    "            #labels_arr = np.array(train_data[\"target\"][i * batch_size:(i + 1) * batch_size])\n",
    "            #labels_arr = tf.cast(text_arr, dtype=tf.int32)\n",
    "            yield train_questions, np.array(train_data[\"target\"][i * batch_size:(i + 1) * batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff90508-49b1-4594-ac69-606eada20c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator for validation data\n",
    "batch_size = 256\n",
    "\n",
    "def val_gen(val_data):\n",
    "    num_batches = math.ceil(len(val_data) / batch_size)\n",
    "    while True: \n",
    "        val_data = val_data.sample(frac=1.)\n",
    "        for i in range(num_batches):\n",
    "            val_questions = []\n",
    "            batch = val_data.iloc[i * batch_size: (i + 1) * batch_size, 1]\n",
    "            for question in batch:\n",
    "                val_questions.append(glove_embedding(question))\n",
    "        val_questions = np.array(val_questions, dtype = object)\n",
    "        val_questions = pad_sequences(val_questions, padding = 'post', dtype = 'float32', maxlen = 100)\n",
    "        yield val_questions, np.array(val_data[\"target\"][i * batch_size:(i + 1) * batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "9d06c404-197b-48d0-bd87-531cb1d52614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mg is a combination of word embedding and target value. \n",
    "train_generator = train_gen(train_data)\n",
    "val_generator = val_gen(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "3defec1e-7f45-40b5-a1fc-473bb55877e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 245s 241ms/step - loss: 0.1427 - accuracy: 0.9475\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 265s 265ms/step - loss: 0.1201 - accuracy: 0.9539\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 268s 268ms/step - loss: 0.1161 - accuracy: 0.9548\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 264s 264ms/step - loss: 0.1112 - accuracy: 0.9563\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 263s 263ms/step - loss: 0.1097 - accuracy: 0.9570\n"
     ]
    }
   ],
   "source": [
    "# training on mg, tuples of embedding and target\n",
    "history = model.fit(train_generator, epochs=5, steps_per_epoch=1000, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "3d39a173-faff-4112-815a-2c36d1d32042",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add((tf.keras.layers.LSTM(64,return_sequences=True)))\n",
    "model.add(tf.keras.layers.GlobalMaxPool1D())\n",
    "model.add(tf.keras.layers.Dense(32,activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "f32e2b2e-985e-4bdb-8bad-c57a7149204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(0.001), loss=\"binary_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48e03a4-cd7c-46ca-98d3-ccdd2d7814b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator, epochs=10, steps_per_epoch = 1000, validation_data = val_generator, validation_steps = 500, verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df92470-b4ac-475f-8a18-6915d8708fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebda643-e3ae-4b0f-9673-06d430bf179d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273/273 [==============================] - 26s 91ms/step - loss: 0.2424 - accuracy: 0.9363\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for i in range(12): # 12 files of training data\n",
    "        data_filename = \"train_data_\"+str(i)+\".npy\"\n",
    "        train_data = np.load(file = data_filename, allow_pickle = True)\n",
    "        \n",
    "        labels_filename = \"train_labels_\"+str(i)+\".npy\"\n",
    "        train_labels = np.load(file = labels_filename, allow_pickle = True)\n",
    "        \n",
    "        model.fit(train_data, train_labels, batch_size = 256, epochs = 1)\n",
    "        \n",
    "        # hosuekeeping\n",
    "        del train_data\n",
    "        del train_labels\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d097b7d1-22dc-4812-ad38-51e17b7637d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfd9d7d3-839d-4870-918a-86096641128a",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a8432-915d-42f9-af2d-4c01b157cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15): \n",
    "    \n",
    "  # Create the confustion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n",
    "    n_classes = cm.shape[0] # find the number of classes we're dealing with\n",
    "\n",
    "  # Plot the figure \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors represent how 'correct' a class is, darker == better\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "  # Check for list of classes\n",
    "    if classes:\n",
    "        labels = classes\n",
    "    else:\n",
    "        labels = np.arange(cm.shape[0])\n",
    "  \n",
    "  # Label the axes\n",
    "    ax.set(title=\"Confusion Matrix\",\n",
    "         xlabel=\"Predicted label\",\n",
    "         ylabel=\"True label\",\n",
    "         xticks=np.arange(n_classes), # create enough axis slots for each class\n",
    "         yticks=np.arange(n_classes), \n",
    "         xticklabels=labels, # axes will labeled with ints\n",
    "         yticklabels=labels)\n",
    "  \n",
    "  # Make x-axis labels appear on bottom\n",
    "    ax.xaxis.set_label_position(\"bottom\")\n",
    "    ax.xaxis.tick_bottom()\n",
    "\n",
    "  # Set the threshold for different colors\n",
    "    threshold = (cm.max() + cm.min()) / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a255eda2-7fa5-458a-aa55-606aaeb54a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_val = model.predict(x=val_embedding, verbose = 0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96461b5c-efe2-4260-8597-d7a609745bf1",
   "metadata": {},
   "source": [
    "rounded_predictions_val = np.argmax(predictions_val, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c1c30c-add7-48f5-9f2a-ab5a4826cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(val_label, rounded_predictions_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5160fba8-a87a-4059-940c-c8d846c7c839",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(y_true=val_label, y_pred=rounded_predictions_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b823ded-1978-41a8-8703-1ab183c2784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the percentage of correct predictions per class\n",
    "averages = matrix.diagonal()/matrix.sum(axis=1)\n",
    "print(\"Array containing percentage of true predictions by class: \\n\", averages)\n",
    "\n",
    "# Get average percentage of correct predictions across classes \n",
    "sum = 0\n",
    "for average in averages:\n",
    "    sum += average\n",
    "total_average = sum/2\n",
    "print(\"Average percentage of true predictions: \", total_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f51928-b7b4-4558-a62d-3b24ec477efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_0 = np.load(file = \"train_data_0.npy\")\n",
    "train_data_1 = np.load(file = \"train_data_1.npy\")\n",
    "train_data_2 = np.load(file = \"train_data_2.npy\")\n",
    "train_data_3 = np.load(file = \"train_data_3.npy\")\n",
    "train_data_4 = np.load(file = \"train_data_4.npy\")\n",
    "train_data_5 = np.load(file = \"train_data_5.npy\")\n",
    "train_data_6 = np.load(file = \"train_data_6.npy\")\n",
    "train_data_7 = np.load(file = \"train_data_7.npy\")\n",
    "train_data_8 = np.load(file = \"train_data_8.npy\")\n",
    "train_data_9 = np.load(file = \"train_data_9.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2279d2e6-2b4e-4e6a-a343-66b4d0c0bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(16,(4,4),strides=(2,2),input_shape = (80, 60, 32), padding = \"valid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdaad628-0e67-45b4-b954-1536a342a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(20,(7,7),strides=(3,3),input_shape = (300,200,3), padding = \"valid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7bdb75f-19e1-4830-8d79-6e8ca59602f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 39, 29, 16)        8208      \n",
      "=================================================================\n",
      "Total params: 8,208\n",
      "Trainable params: 8,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7897e9-3e80-402b-8d60-c005a2e39707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "python3-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
